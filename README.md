# Prog2kakushin
作成した課題プログラムを公開する場所。
## プログラム1について(12/19 Pandasを用いたプログラム)
今回作成したのは疑似的な辞書作成プログラムです。  
文章を読む時、特に教科書やネット上の参考情報などを読んでいるときに、  
単語の意味だけまとめたことはありませんか？  
このプログラムではテキストデータから自分でやると意外と面倒なこの作業をある程度簡単に、  
かつ重要度を把握しながら行えます。  
このプログラムでは  
1.テキストファイルを読み込み\nなどを削除  
2.任意の品詞に対してその単語とそれぞれの出現回数をカウント  
3.単語と出現回数をデータフレーム化  
4.回数を基に降順でソート  
5.指定した件数だけ上位のデータを抽出
6.意味や読みなど必要なものをデータに追加  
7.データをcsvファイル化、表示  
を行います。　　

テキストデータを用いてプログラムを動作させるため、ファイルを用意してください。  
使用できるファイルは.txtファイルのみです。
事前準備についての説明です。  
```sh
!pip install janome
!pip install Tokenizer
```
  
形態素解析を用いて品詞解析を行うため上記2つをインストールします。  

```sh
%%bash
cat <<EOF > input3.txt
#テキストを入力
EOF
```
このコードでGoogle Colab上ならテキストファイルを作成できます。  
ドライブから個人的に用意しても構いません。  
※上記2つの準備はColab上にも記載しています。  

プログラムを実行した際の出力例です。  
今回はこのREADMEの文章の一部を用いて実行しています。  

| |単語|回数|読み|
|:----|:----|:----|:----|
|0|作成|2|さくせい|
|1|プログラム|2|ぷろぐらむ|
|2|今回|1|こんかい|
|3|の|1|の|
|4|疑似|1|ぎじ|
  
名詞の上位5件を読みデータを追加して表示しています。  

工夫した点は  
・文字化け対策として2種類の文字コードでcsvファイルを出力する  
・例外処理を用意して使用しやすいようにする  
・品詞を指定できるようにする(通常は名詞を想定)  
・単語にフォーカスして辞書のようにする  

といった点です。  
文章要約はあるけれど単語をまとめることはあまりされてないのではと思い作成しました。  
今後さらに改造するとすれば、  
・意味検索などができるAPIの導入  
・形態素解析のより有効的な活用  
・ファイルの導入の簡単化  
などすれば面白いかもしれません。


## 01/09 Numpyを用いたプログラム
Numpyを用いる。(work2)
## 01/16 画像処理を用いたプログラム
OpenCV,Turtleなどを用いる。(work3)
